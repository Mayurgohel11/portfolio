#Personal details

Name:		Mayur

Availability: 	January 2025	

Role:		Senior Data Architect / Engineer

Residence: 	Ahmedabad, India

Language:	English	

#Profile
Mayur is an energetic IT specialist with over 13 years of experience delivering excellent organizational performance, optimizing operations, and automating processes. Mayur has extensive experience with data architecture, design, and development of BI/ETL solutions. He possesses proven Data Architect, Data Engineering, and Modelling skills, as well as hands-on experience in establishing, interpreting, and simplifying data to enable high-growth operations with significant impact. He is skilled in facilitating all activities related to the design, development, implementation, migration, administration, and support of BI/ETL processes for large-scale data warehouses using Tableau/Power BI. Mayur also has experience with Azure, AWS, Python, SSAS, SSIS, MongoDB, PostgreSQL, MS SQL, MySQL, Oracle, and Cosmos DB.

Skills
 
**Databases**\n
MongoDB	★ ★ ★ ★ ★ 
PostgreSQL	★ ★ ★ ★ ★
T-SQL	★ ★ ★ ★ ★
MySQL	★ ★ ★ ★ ★
PL/SQL	★ ★ ★ ★ ★
NoSQL 	★ ★ ★ ★ ★
MS SQL	★ ★ ★ ★ ★
Oracle	★ ★ ★ ★ ★

**CloudOps/DevOps**
Azure SQL	★ ★ ★ ★ ★
CosmosDB	★ ★ ★ ★ ★
Azure Synapse	★ ★ ★ ★ ★
Data Factory	★ ★ ★ ★ ★
Azure Function	★ ★ ★ ★ ★
Blob Storage	★ ★ ★ ★ ★
ADLS	★ ★ ★ ★ ★
Stream Aalytics	★ ★ ★ ★ ★
Data Explorer	★ ★ ★ ★ ★
Data Catalog	★ ★ ★ ★ ★
Data Bricks	★ ★ ★ ★ ★
Data Migration	★ ★ ★ ★ ★
Azure DevOps	★ ★ ★ ★
Azure CI/CD	★ ★ ★ ★ ★
Logic Apps	★ ★ ★ ★ ★
Azure HDInsight	★ ★ ★ ★

**Software Development**
Python 	★ ★ ★ ★ ★
Pandas	★ ★ ★ ★
Numpy 	★ ★ ★ 
Scala	★ ★ ★
PHP	★ ★ ★
.NET	★ ★ ★

**Data Processes**
Data Modelling	★ ★ ★ ★ ★
SSAS	★ ★ ★ ★ ★
Apache Kafka	★ ★ ★ 
SSIS 	★ ★ ★ ★ ★
Data Analysis	★ ★ ★ ★ ★
ETL	★ ★ ★ ★ ★
Data Mining	★ ★ ★ ★
Other
Power BI	★ ★ ★ ★ ★
SQL Server	★ ★ ★ ★ ★
Tableau	★ ★ ★ ★ ★
Pentaho	★ ★ ★ 
Kimball	★ ★ ★
ARM(Azure res.)	★ ★ ★ ★ ★
DevOps	★ ★ ★ ★ 
Linux	★ ★ ★ ★
Git 	★ ★ ★ ★
CI/CD	★ ★ ★ ★ 
Snowflake	★ ★ ★ ★
Erwin	★ ★ ★
Lucide Chart	★ ★ ★ ★ ★
Azure KeyVault    ★ ★ ★ ★ ★	
 





**Certifications**
AWS Certified Database - Specialty (AWS DBS C01) – 2023

Oracle8i – 2007 
Education
Bachelor of Engineering in Information Technology  


**Specializations**
1.	Database Development: 5/5
With Over 10 years of experience
I worked on many databases platform like MSSQL, Oracle, MySQL, Postgres, and MongoDB. I have Designed and implemented scalable and efficient database solutions for diverse business applications. Developed large scale database to handle big applications which are utilized by multiregional and millions of people. Proven work on complex stored procedures, triggers, and queries to ensure optimal data retrieval and manipulation.
2.	Database Design/Architect: 5/5
With Over 10 years of experience
Led the design and architecture of database systems, considering scalability, security, and data integrity. Implemented data modelling techniques to create logical and physical database designs that align with business requirements.

3.	Data Migration: 5/5 
With Over 8 years of experience
Worked on multiple data migration projects with expertise on different approaches like homogeneous migration, heterogeneous migration, excel/csv files to relational database migration, relation database to files migration, module migration, functional migration and lot more. Worked on different migration tools like AWS DMS (data migration services), AWS SCT (Schema conversion tool), Azure Migration services, Azure ADF, SSIS and Pentaho.

4.	DBA Activities: 5/5
With Over 7 years of experience
Performed routine database maintenance tasks, including backups, restores, and database security management. Monitored and optimized database performance to ensure high availability and responsiveness.
5.	BI Reports: 5/5
With Over 7 years of experience
Designed and developed Business Intelligence (BI) reports to provide actionable insights for strategic decision-making. Integrated data from various sources to create comprehensive and visually appealing reports and dashboards.
6.	Data Engineering: 5/5
With Over 4 years of experience
Engineered ETL processes to seamlessly extract, transform, and load data from diverse sources into the data warehouse. Implemented data pipelines for real-time and batch processing to support analytical and reporting needs.
7.	Data Analysis: 4/5
With Over 4 years of experience
Conducted in-depth data analysis to identify trends, patterns, and correlations, providing valuable insights for business stakeholders. Utilized statistical methods and machine learning algorithms to derive meaningful conclusions from complex datasets.
8.	Data Analytics: 4/5
With Over 7 years of experience
Applied advanced analytics techniques to uncover hidden patterns and trends in large datasets. Developed predictive models and statistical analyses to support data-driven decision-making.

9.	Database Modelling: 4/5
With Over 5 years of experience
Created and maintained detailed data models, ensuring alignment with business processes and objectives. Employed ER modelling techniques to capture relationships and dependencies between data entities.
10.	DB Troubleshooting: 5/5
With Over 8 years of experience
Diagnosed and resolved database issues promptly, minimizing downtime and ensuring data integrity. Implemented effective troubleshooting strategies for identifying and addressing performance bottlenecks.
11.	DB Performance Tuning: 5/5
With Over 8 years of experience
Conducted performance tuning activities to optimize database queries and enhance overall system performance. Implemented indexing strategies and configuration adjustments to improve query response times and resource utilization.
12.	Data Governance: 4/5
With Over 3 years of experience
Developed and implemented a comprehensive data governance framework, defining policies, standards, and procedures for data management across the organization. Implemented data quality initiatives to ensure accuracy, completeness, and consistency of data, reducing errors and improving overall data reliability.
13.	ELT & ETL: 5/5
With Over 5 years of experience
Implemented ELT processes to extract data from source systems, load it into the data warehouse, and perform transformations within the database for improved performance. Utilized tools and frameworks to orchestrate and automate the ELT workflows, streamlining the data integration process.
Designed and developed ETL processes to extract data from diverse sources, transform it into a consistent format, and load it into the target data warehouse or data mart. Integrated data cleansing, validation, and enrichment techniques within ETL workflows to ensure data quality and integrity.
14.	Data Warehouse: 4/5
With Over 6 years of experience
Developed and implemented a comprehensive data governance framework, defining policies, standards, and procedures for data management across the organization. Implemented data quality initiatives to ensure accuracy, completeness, and consistency of data, reducing errors and improving overall data reliability. Led the design and implementation of a scalable and high-performance data warehouse architecture to support business intelligence and analytics requirements. Established and maintained dimensional data models, ensuring efficient storage and retrieval of data for analytical processing.




**Employment History**

Employer: 	Wypoon Technologies B.V.
Period:	Jan 2025 – Present
Role:	Senior Data Engineer

Mayur is available for secondment through Wypoon Technologies.


Role:	Team Lead Data Engineering / Data Architect
Period:	June 2021 – Dec 2024	
Employer:	SIMFORM SOLUTIONS

Simform is a product engineering firm dedicated to solving intricate software challenges. Simform guides companies in technology selection, architecture design, and ensure the successful completion of software projects.

Key Projects and Contributions:
Project: US Connect
For this project, Mayur worked alongside .NET, QA, and front-end developers to resolve the client’s data movement, data migration, data validation, and data storage needs. Mayur assisted his team with data mapping, performance tuning, ETL testing, analytics reports, and new enhancements.

Project: ConnectHQ / Global Connect
For this project, Mayur created a database which supports Global Connect operators (affiliates) and third-party operators. This database functions in a multi-tenant / sub-tenant architecture and supports muti-currency and muti-language usage. Azure CosmosDB database for better scalability and performance of application.

Responsibilities:
•	Served as Team Lead for the Transaction Data Monitoring team of 15 engineers.
•	Created unique schemas for each project and implemented multi-tenant architecture with Azure Cosmos DB Database.
•	Used Python for data analysis and data migration purposes.
•	Migrated Python Django queries into SQL queries.
•	Created equivalent database logins so that the appropriate data access policies could be applied by restricting database users' database access. 
•	Evaluated database solutions for efficacy, addressing and escalating issues for maximum functionality.
•	Collaborated with cross-functional teams to create functional, high-performance database architectures.
•	Built optimized data models in Azure Synapse Analytics, ensuring high performance for analytical workloads.
•	Designed data pipelines to read and write to SQL DB using Azure Data Cloud.
•	Analyzed data findings and created user-friendly reports and presentations detailing results to assist the organization in making crucial decisions.
•	Performed Large Data Migration activities from sources such as MSSQL, PostgreSQL, and MySQL.
•	Designed DWH, ETL, and Reporting solutions, Dashboard and Storyboard dashboards based on T-force data to display various KPls and cater to intelligence in building trends, variations, forecast, cross-sell and up-sell dashboards.
•	Created a data modelling dashboard reconciliations and generated mapping based on business requirements and modelling principles.
•	Database Architect / Database modelling from the ground up for OLTP and OLAP systems.
•	Designed and mapped life-cycle data flows and processes, to produce high-performing data-management systems.
•	Carried out Extract, Transform, and Load (ETL) operations to support the data warehouse system.
•	Motivated and empowered team members to assume responsibilities within the team, improve their technical and interpersonal skills, and advance within the organization.
•	Developed and implemented project timelines, monitored progress of the team, and (re-) delegated tasks based on the strengths of each individual, their personal development goals, and the overall needs of the team.

Technologies: AZURE, AWS, MSSQL, Postgres, Python (Django, Boto3, Pandas, NumPy, Matplotlib), Data Factory, AWS Glue, Cosmos DB, Power BI, CI/CD, Azure Function, Azure Databricks, Azure Synapse, AWS Serverless, AWS DynamoDB, AWS Neptune Database, AWS Aurora, MongoDB, Snowflake. 


Role:	Lead Data Engineer (Assistant Manager)
Period:	March 2021 – Jun. 2021		
Employer:	Accelerated Growth

Accelerated Growth (AG) provides entrepreneurial organizations with the people and tools to achieve scalable growth by providing companies with expertise in managed accounting, finance transformation, technology and data.

Key Projects and Contributions:
Project: Bunker Labs 
Mayur was solely responsible for all data-related tasks in this project. Mayur prioritized responsibilities such as database administration, ETL, data migration, and data visualization while also focusing on data collection, data storage, and reporting.

Project: Cubii
For this project, Mayur was entirely responsible for all data related endeavors including but not limited to data migration, data storage on Datawarehouse, and reporting activities such as creating dashboards to facilitate business decisions. Mayur was also responsible for database management, ETL, data migration and data visualization. 

Responsibilities:
•	Developed a data pipeline to extract, transform, and load data into a Data Warehouse resulting in a 40% improvement in data processing times.
•	Implemented data quality checks to ensure data accuracy and integrity.
•	Designed and implemented an automated monitoring and alerting system to detect and resolve data processing errors.
•	Collaborated with cross-functional teams to ensure alignment with business requirements.
•	Identified and analyzed skill shortages in the team and pod and contributed to the development of readiness plans and content in collaboration with the readiness and training teams.
•	Worked closely with Product Owners to develop and maintain project backlogs and remove problems and team impediments to ensure that the highest business value user stories are delivered to the customers.
•	Supported strategic direction by identifying success factors and measurements of continuous improvement and changes in culture through conducting surveys that provide insight to teams’ and customers’ overall satisfaction, includes Net Promoter Score (NPS) survey question.
•	Developed new Tableau and Excel dashboards that provided greater visibility into critical areas such as Cybersecurity, Business Impacting Events, and Project and Portfolio Management.
•	Played a key role in exception handling at various levels e.g., source level without logging, in-mapping exception with logging, with/without error records and PLSQL coding- views, stored procedures, functions, triggers, sequences and db links.
•	Effectively managed code promotion from development to UAT and UAT to productions.
•	Played a pivotal role in UNIX scripting for automation and file handling, configuration settings and environment refreshing.
•	Successfully implemented Mapplets, UDFs, Control tables, Looping in Informatica, Dynamic parameter file generation.
•	Facilitated production releases by timely uploading of implementation plan, deployment guide, release notes, Runbooks.
•	Extended key assistance in providing warranty support for production go-lives and providing L3 support for production issues. Drove daily status calls with all the stakeholders, walkthrough with production support teams before releases, technical clarification and knowledge sharing sessions.
•	Used Lookup to get metadata activity in order to collect the data from Database and Azure blob storage.
•	Used PowerBI tools to run DAX queries and Power BI Report for Service Management.
•	Performed Alteryx Data Cleansing for Service Management
•	Escalated, monitored, and tracked problems in appropriate trouble management systems to ensure timely resolution.
•	Gathered specifications from service owners, documented requirements, and delivered product releases to meet business needs using the Rave ticketing tool.

Technologies: Azure, AWS, Databricks, Redshift, Postgres, MSSQL, Power BI, ADF, Python (Boto3, Pandas, NumPy, Matplotlib), API, Postman


Role:	Sr. Data Engineer
Period:	Nov. 2018 – Mar. 2021		
Employer:	Trellissoft

Trellissoft helps start-up and product companies with building innovative and award-winning products and modernizing their current technology products.

Key Projects and Contributions:
Project: Empereon Constar 
For This project, Mayur was aided Database development, DB architect and Business Intelligence reports. Mayur was responsible for the end-to-end process where data collecting, data moving, data archival, data store in to warehouse and data presentation by Power BI tool. Mayur was also responsible for performance of data movement, ETL and overall application.

Project: Carepoint
For this project, Mayur was aided in a large data migration from multiple client sources to target database. Mayur was personally responsible for end-to-end unit and integration testing and enhancing processes through automation or improvement tasks. He also monitored data ingestion and consumption traffic, job completion, failure statuses, and supervised the creation of reports and dashboards. Mayur was worked with 15 people team and manage the work with proper collaboration, meetings, task allocations and brainstorming on each topic.

Responsibilities
•	Collected and summarized data from API calls, MongoDB, and Azure blob storage for the CarePoint application, transferring this data into staging areas and then into the target Postgres Database.
•	Achieved an end-to-end pipeline using Azure Data Factory, leveraging Azure Synapse for data warehousing and generating insights through PowerBI.
•	Developed databases from the ground up for specific module enhancements and integrated these with the existing system.
•	Tasked with finalizing project scope and prioritizing through Requirement Gathering and analysis, working closely with Business Analysts and users.
•	Monitored data ingestion, traffic, job outcomes, and oversaw the creation of reports and dashboards.
•	Played a key role in designing ETL solutions, securing design approvals from Business Analysts and users.
•	Employed Pentaho for data storage, focusing on enhanced reporting, scalability, and robust security, especially for sectors like online casinos where data security is paramount.
•	Managed operational risks, encompassing identification, assessment, mitigation, and control measures.
•	Engaged with customers regarding payment delinquencies, promoting on-time payments and devising payment plans for good credit standing.
•	Maintained a supervisory role over Data Source Owners and Data Consumers, ensuring optimal data management.

Technologies: Azure, AWS, Django, MSSQL, Postgres, Data Factory, Python, Pandas, Power BI, Pentaho, Django Framework, Python Cron jobs, MongoDB, Jupiter Notebook


Role:	Sr. SQL Developer
Period:	May 2018 – Nov 2018		
Employer:	MBAF (Taken over by BDO USA in 2021)

MBAF is an accounting and advisory firm which offers assurance, tax and accounting, advisory, technology consulting and private wealth management services to a variety of organizations.

Project: PaperSave
PaperSave is a hybrid AP automation and document management solution for on-premises or cloud use.

Core Responsibilities:
•	Worked on product releases, implementing features weekly and applying database changes on production servers. This process involved feasibility checks, testing, and direct feedback provision to customers.
•	Enhanced the "papersave" product by adding columns to track flags and notes, facilitating integration with Office 365. Introduced functions to calculate and display values post-integration with Office 365.
•	Produced various document-related reports like upload, download, modifications, types, and feasibility using SSRS, ensuring customers received them promptly.
•	Integrated the product with Office 365, preceded by data migration activities.
•	Initially utilized SSIS for ETL job generation, but later adopted the ADF pipeline to migrate old data into a new application.
•	Engaged in SQL Server Analysis Services (SSAS) operations, offering Online Analytical Processing (OLAP) and data mining functionalities. This empowered internal stakeholders to process large data sets, derive insights, and inform business decisions.
•	Kept a close watch on data ingestion processes, consumption traffic, job outcomes, and actively supervised the generation of reports and dashboards.
•	Ensured the security of financial transactions by implementing role-based access and robust data encryption methods.

Technologies: AWS, MSSQL, SSIS, SSRS, API, .NET, Office 365, Visual Studio, AZURE


 Role:	BI Analyst/SQL Developer 
Period:	Dec 2015 – Apr 2018		
Employer:	Universal-Software


Key Projects and Contributions:
Project: XJAIL (Inmate management system)
Jail Management is a comprehensive and full feature inmate management system to improve the effectiveness of the facility. It enables the users to monitor and track all records related to inmates’ activities. It is developed to improve the effectiveness of day-to-day operations. 

For This project, Mayur was responsible for database migration, data validation /testing and data presentation and for new agencies those who are interested to purchase this product for their jail which is in different region in US. Mayur used SQL scripts to manage the migration and submit to the client so that client can run those migration on production server. Mayur was responsible for errors and issues during the migration, and he needs to be available during the down time and until the migration was not finished. Mayur needs to give proper support and instructions to clients during the migration over the video call.  

Project: XRMS (Record Management System)
The Record Management System is the most effective way to organize, track and access the vast amount of information that flows through your departments during an incident or investigation. It enables officers and clerical personnel to prepare, search and save various types of important information for law enforcement agencies. RMS has full feature modules such as incidents, arrests, citations, warrants, case management, field contacts, and other operation-oriented records. RMS is an agency-wide system that provides for the storage, retrieval, retention, manipulation, archiving, and viewing of information. 

For This project, Mayur was responsible for database development, data security, data validation and data management. Mayur was responsible for data encryption and usability of data in application side without any interruption. Mayur needs to closely work with dot net developer, project manager and QA guys to achieve this goal. In this project Mayur needs to play DBA activities as well like data storage, data backup management, data restore/archival and performance of the application.



Project: XCAD (US Public safety 911 Calls
For This project, Mayur was responsible for data collection, data security, data validation, Data migration and data management. Mayur was responsible for BI reports which are created by pentaho tool. Mayur needs to closely work with dot net developer, project manager and QA guys to achieve this goal.
Project: UNIQ EMR (Inmate Hospitality management)
Mayur aided Database development, data migration and BI reports. Mayur was responsible for HL7 data movement and followed the best practice to share that data with another agencies. Mayur was responsible for the end-to-end process where data collecting, data moving, data archival, data store in to warehouse and data presentation by PowerBI tool. Mayur was also responsible for performance of data movement, ETL and overall application.

Responsibilities: 
•	Ensured secure storage of all public safety product data using data masking and encryption techniques.
•	Managed major data migration activities for all products while providing customer support.
•	Conducted data migrations using SQL scripts and the SSIS tool.
•	Handled DBA activities, including data storage, backup management, restoration/archival, and application performance optimization.
•	Addressed errors and issues during migration, ensuring availability during downtimes until migration completion.
•	Executed ETL and Datawarehouse setups as agencies purchased and began utilizing the product.
•	Regularly harvested data through API calls, staged it, and then transferred it into the data warehouse using SSIS.
•	Leveraged SSIS and SSRS reports to automate daily validation processes, status updates, and report generation.
•	Implemented data encryption methods on the MSSQL database to enhance data security for public safety products.
•	Demonstrated expertise in software development methodologies, including RUP, Agile, Extreme Programming, SDLC, and Waterfall.
•	Designed and maintained Enterprise Data Models for major data initiatives.
•	Set and championed data modeling standards and best practices.
•	Led the database team, overseeing three members, while liaising with other development teams for product support and maintenance.

Technologies: AZURE, AWS, MSSQL, .Net, API, Pentaho, SSIS, SSRS, Oracle, Linux, Windows 7, API, LINQ Framework




